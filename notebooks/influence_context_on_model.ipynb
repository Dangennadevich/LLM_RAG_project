{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install 'torch==2.3.1' 'accelerate==0.31.0' 'flash_attn==2.5.8' 'transformers==4.43.0' Minio dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f93843ca6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "YANDEX_CLOUD_ACCESS_KEY = os.getenv(\"YANDEX_CLOUD_ACCESS_KEY\")\n",
    "YANDEX_CLOUD_SECRET_KEY = os.getenv(\"YANDEX_CLOUD_SECRET_KEY\")\n",
    "\n",
    "BUCKET_NAME = 'rag-project' # S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Init def, model, tokenizer, Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe4b11108254f7894a5ac6784a5ad6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879400652d96427f9c1a533ac49aed47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   3%|3         | 157M/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e594071265cf4b58a6bfcd1938c3d089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c596d52d0584e88bca058827ea64338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd175b2b97d451d8d573956ef6d77be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a002bbd637948adb21d09c09e2a7826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11c19e76d434e86a0cecc22d3a1c626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a9d8d543f546a6a9f736dde129f16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffa60ccd3d54c82b14e9edfb5e57b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d9f60fdd254fd6a40e2680ced7f4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init tokenizer and model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(user_query: str, rag_context: str) -> str:\n",
    "    \"\"\"Create prompt with/whiout RAG-content\"\"\"\n",
    "\n",
    "    if rag_context:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You - AI-asistant with access to documents. \"\n",
    "                    \"Use the provided context to respond..\\n\\n\"\n",
    "                    f\"Context:\\n{rag_context}\"\n",
    "                )\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ]\n",
    "    \n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize_special_tokens=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_truncate(text: str, max_tokens: int = 4096) -> str:\n",
    "    \"\"\"Cut text\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return tokenizer.decode(tokens[:max_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(user_query, rag_context=None):\n",
    "    \"Model inference with user_query or user_query + rag_context\"\n",
    "    prompt = build_rag_prompt(user_query=user_query, rag_context=rag_context)\n",
    "\n",
    "    output = pipe(prompt, **generation_args)\n",
    "\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Minio(\n",
    "    \"storage.yandexcloud.net\",\n",
    "    access_key=YANDEX_CLOUD_ACCESS_KEY,\n",
    "    secret_key=YANDEX_CLOUD_SECRET_KEY,\n",
    "    secure=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from s3\n",
    "client.fget_object(\n",
    "    bucket_name=BUCKET_NAME, \n",
    "    object_name='pdf_2412_19437v1_prompt1.pkl',\n",
    "    file_path='/content/pdf_2412_19437v1_prompt1.pkl'\n",
    "    )\n",
    "\n",
    "with open(\"/content/pdf_2412_19437v1_prompt1.pkl\", \"rb\") as file:\n",
    "    pdf_2412_19437v1_prompt1 = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate \n",
    "\n",
    "RAG based on [DeepSeek-v3 Technical Report](https://arxiv.org/abs/2412.19437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "| Model       | Accuracy / Percentile (%) |\n",
      "|-------------|---------------------------|\n",
      "| DeepSeek-V3 | 75.9                      |\n",
      "| DeepSeek-V2 | 59.1                      |\n",
      "| Qwen2.5-72B-Inst | 66.5                   |\n",
      "| Llama-3.1-405B-Inst | 76.3                 |\n",
      "| GPT-4a-0513 | 74.6                     |\n",
      "| Claude-3.5-Sonnet-1022 | 56.8               |\n",
      "Table 1 | Comparison of Performance Metrics\n",
      "| Metric          | DeepSeek-V3 | DeepSeek-V2 | Qwen2.5-72B-Inst | Llama-3.1-405B-Inst | GPT-4a-0513 | Claude-3.5-Sonnet-1022 |\n",
      "|-----------------|------------|------------|--------------------|---------------------|------------|----------------------|\n",
      "| MMLU-Pro        | 75.9       | 59.1       | 66.5               | 76.3                | 74.6       | 56.8                 |\n",
      "| CPQA-Diamond    | 59.1       | 41.3       | 49.9               | 48.9                | 47.6       | 39.2                 |\n",
      "| MATH 500        | 90.2       | 74.7       | 80.0               | 76.3                | 74.6       | 60.0                 |\n",
      "| AIME 2024      | 39.2       | 16.7       | 23.31               | 16.0                | 23.6       | 42.0                 |\n",
      "| Codeforces     | 51.6       | 24.2       | 24.3               | 23.6                | 23.6       | 38.5                 |\n",
      "| SWE-bench Verified | 42.0       | 23.824.5 | 23.824.5           | 23.824.5            | 23.824.5 | 38.5                 |\n",
      "This document provides a detailed technical report on DeepSeek-V3, highlighting its architecture, training methods, and performance metrics compared to other models.\n"
     ]
    }
   ],
   "source": [
    "print(pdf_2412_19437v1_prompt1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_context = pdf_2412_19437v1_prompt1[0].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____How many total parameters does DeepSeek-V3 have?____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: Deepseek-v3 have 671b parameters\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "With context:\n",
      "  DeepSeek-V3 has a total of 671 billion parameters.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Without context :\n",
      "  I'm sorry, but I do not have specific information on a model named \"DeepSeek-V3.\" The number of parameters in a neural network model depends on its architecture, including the number and size of its layers, the type of layers (e.g., convolutional, recurrent, fully connected), and other design choices.\n",
      "\n",
      "If \"DeepSeek-V3\" is a model from a research paper, a proprietary system, or a model from a specific software library, you would need to refer to the original source or documentation for the exact number of parameters.\n",
      "\n",
      "If you provide more context or details about the model, I may be able to help you better understand its architecture or guide you on where to find the information.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'How many total parameters does DeepSeek-V3 have?'\n",
    "print('Correct: Deepseek-v3 have 671b parameters')\n",
    "\n",
    "print('-'*150)\n",
    "\n",
    "# whith RAG\n",
    "answer = model_inference(user_query=user_query, rag_context=rag_context)\n",
    "print('With context:\\n', answer)\n",
    "\n",
    "print('-'*150)\n",
    "\n",
    "# Without rag\n",
    "answer = model_inference(user_query=user_query)\n",
    "print('Without context :\\n', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____What was the Accuracy of DeepSeek-V3 and Qwen2.5-72B-Inst based on the technical report on the DeepSeek-V3 model?____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: Deepseek-v3 75.9 VS DeepSeek-v2.5 66.2\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "With context:\n",
      "  DeepSeek-V3 achieved a rating of 75.9% on the MMLU-Pro benchmark. When compared to DeepSeek-V2.5, which scored 59.1%, DeepSeek-V3 showed an increase of 16.8 percentage points. This indicates a significant improvement in performance on the MMLU-Pro benchmark.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Without context :\n",
      "  I'm sorry, but as of my last update in April 2023, I don't have real-time or specific data access, including performance metrics for models like \"deepseek-v3\" on benchmarks such as MMLU-Pro. To find this information, you would need to refer to the latest research papers, model benchmarking results, or the official repositories where these models are documented.\n",
      "\n",
      "If you have access to the relevant data, you can calculate the increase in performance by comparing the scores of \"deepseek-v3\" and \"deepseek-v2.5\" on the MMLU-Pro benchmark. Here's how you could do it:\n",
      "\n",
      "1. Find the score of \"deepseek-v3\" on the MMLU-Pro benchmark.\n",
      "2. Find the score of \"deepseek-v2.5\" on the same benchmark.\n",
      "3. Subtract the score of \"deepseek-v2.5\" from the score of \"deepseek-v3\".\n",
      "\n",
      "The result will give you the increase in performance from \"deepseek-v2.5\" to \"deepseek-v3\".\n",
      "\n",
      "For example, if \"deepseek-v3\" scored 85 and \"deepseek-v2.5\" scored 80 on the MMLU-Pro benchmark, the increase would be:\n",
      "\n",
      "85 (deepseek-v3 score) - 80 (deepseek-v2.5 score) = 5\n",
      "\n",
      "This means there's a 5-point increase in performance from \"deepseek-v2.5\" to \"deepseek-v3\" on the MMLU-Pro benchmark.\n",
      "\n",
      "Please ensure you have the correct and updated data to perform this calculation.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'What rating did deepseek-v3 get on the MMLU-Pro benchmark and what is the increase relative to deepseek-v2.5?'\n",
    "print('Correct: Deepseek-v3 75.9 VS DeepSeek-v2.5 66.2')\n",
    "\n",
    "print('-'*150)\n",
    "\n",
    "# whith RAG\n",
    "answer = model_inference(user_query=user_query, rag_context=rag_context)\n",
    "print('With context:\\n', answer)\n",
    "\n",
    "print('-'*150)\n",
    "\n",
    "# No RAG\n",
    "answer = model_inference(user_query=user_query)\n",
    "print('Without context :\\n', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 | Training Costs of DeepSeek-V3\n",
      "| Training Costs | Pre-Training | Context Extension | Post-Training | Total |\n",
      "|----------------|---------------|-------------------|----------------|-------|\n",
      "| in H800 GPU Hours | 2664K         | 119K              | 5K             | 2788K |\n",
      "| in USD          | $5.328M       | $0.238M           | $0.01M         | $5.576M |\n",
      "Table 1 | Training Costs of DeepSeek-V3, Assuming the Rental Price of H800 is $2 per GPU Hour.\n",
      "We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the strongest open-source base model currently available, especially in code and math. Its chat version also outperforms other open-source models and achieves performance comparable to leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard and open-ended benchmarks.\n",
      "Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware. During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post-training, DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n",
      "- We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "- Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computation-communication overlap. This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.\n",
      "- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "- We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(pdf_2412_19437v1_prompt1[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_context = pdf_2412_19437v1_prompt1[4].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____How much did the Pre-Training stage cost in 800 GPU Hours, as well as in USD? Also give an estimate of the cost of full-time education____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "With context:\n",
      "  The DeepSeek-V3 pre-training stage requires 180,000 GPU hours (180K GPU hours) to train on each trillion tokens. The cluster mentioned in the context consists of 2048 H800 GPUs. This information is derived from the context provided, which states that \"training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours.\"\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Without context :\n",
      "  The DeepSeek-V3 pre-training stage, like any machine learning model training, depends on various factors such as the complexity of the model, the size of the dataset, the batch size, the learning rate, and the efficiency of the implementation. Therefore, it's not possible to provide a specific number of GPU hours required per trillion without detailed information about these factors.\n",
      "\n",
      "To estimate the GPU hours required, you would need to consider:\n",
      "\n",
      "1. Model complexity: Larger models with more parameters will generally require more computational resources.\n",
      "2. Dataset size: The size of the dataset and the number of epochs (iterations over the dataset) will impact the training time.\n",
      "3. Batch size: The number of samples processed before updating the model's weights.\n",
      "4. Learning rate: The step size used during optimization.\n",
      "5. Hardware efficiency: The specific GPUs and their utilization efficiency.\n",
      "\n",
      "Once you have an estimate of the GPU hours required, you can calculate the number of GPUs needed in a cluster by dividing the total GPU hours by the number of hours each GPU can run.\n",
      "\n",
      "For example, if you estimate that the DeepSeek-V3 pre-training stage requires 10,000 GPU hours and each GPU can run for 100 hours, you would need:\n",
      "\n",
      "10,000 GPU hours / 100 hours per GPU = 100 GPUs\n",
      "\n",
      "So, you would need a cluster with at least 100 GPUs to complete the pre-training stage within the estimated time frame.\n",
      "\n",
      "Keep in mind that this is a simplified example, and actual calculations would require more detailed information about the training process. Additionally, factors such as data transfer times, multi-GPU synchronization, and other overheads can also affect the total GPU hours required.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'How much does the DeepSeek-V3 pre-training stage requires GPU hours on each trillion? How many GPUs in 1 cluster?'\n",
    "print('Correct: During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs.')\n",
    "\n",
    "print('-'*150)\n",
    "\n",
    "# whith RAG\n",
    "answer = model_inference(user_query=user_query, rag_context=rag_context)\n",
    "print('With context:\\n', answer)\n",
    "\n",
    "print('-'*150)\n",
    "\n",
    "# No RAG\n",
    "answer = model_inference(user_query=user_query)\n",
    "print('Without context :\\n', answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
